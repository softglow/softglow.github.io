<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[softglow's notebook]]></title>
  <link href="http://softglow.github.io/atom.xml" rel="self"/>
  <link href="http://softglow.github.io/"/>
  <updated>2013-11-14T21:25:30-05:00</updated>
  <id>http://softglow.github.io/</id>
  <author>
    <name><![CDATA[softglow]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DSP Emulators]]></title>
    <link href="http://softglow.github.io/blog/2013/11/14/dsp-emulators/"/>
    <updated>2013-11-14T21:15:00-05:00</updated>
    <id>http://softglow.github.io/blog/2013/11/14/dsp-emulators</id>
    <content type="html"><![CDATA[<p>I was reading around byuu&rsquo;s forum (technically, preparing to make sure my
question regarding the accuracy of byuu&rsquo;s SMP/DSP vs. blargg&rsquo;s was in the
right sub-forum) when I came upon the answer to my question.</p>

<p>To summarize into a a perfect viral posting for social sites&hellip;</p>

<h2>TIL: 3 Amazing Facts About SNES Audio Emulators</h2>

<ol>
<li>blargg&rsquo;s DSP is 100% pure awesome.</li>
<li>byuu&rsquo;s SMP is better than blargg&rsquo;s, though.</li>
<li>byuu&rsquo;s <em>main</em> SMP only has style points over the <em>alt</em> SMP core.  E.g. it
emulates the <code>TEST</code> register which nobody uses.</li>
</ol>


<h2>So was I wasting my time?</h2>

<p>Partly.  I think extracting <code>sfc/alt/{smp,dsp}</code> is the way to go, which makes
the deep dive on the main cores mostly irrelevant, but I still got an overview
of how higan is structured and how its audio path fits in.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe I've been wasting my time?]]></title>
    <link href="http://softglow.github.io/blog/2013/11/12/maybe-ive-been-wasting-my-time/"/>
    <updated>2013-11-12T20:17:00-05:00</updated>
    <id>http://softglow.github.io/blog/2013/11/12/maybe-ive-been-wasting-my-time</id>
    <content type="html"><![CDATA[<p>In higan&rsquo;s <code>sfc/alt/dsp</code> directory is the alternate SMP emulation core, used
when the accuracy profile isn&rsquo;t selected.  It apparently depends on a couple
of <a href="http://slack.net/~ant/libs/audio.html">Blargg&rsquo;s audio libraries</a> which
includes&hellip; &ldquo;snes_spc-0.9.0&rdquo;.  Of which blargg boasts:</p>

<blockquote><p>The accurate DSP passes over a hundred strenuous timing and behavior
validation tests that were also run on the SNES. As far as I know, it&rsquo;s the
first DSP emulator with cycle accuracy, properly emulating every DSP
register and memory access at the exact SPC cycle it occurs at&hellip;</p></blockquote>

<p>And of course, <em>this</em> is already written as a library, meant to be used in
another project.  No games required.</p>

<p>My only question is: if <code>snes_spc</code> is <em>that</em> accurate, why does higan not use
it in accuracy mode for its own accuracy profile?  Is byuu&rsquo;s DSP somehow more
accurate?  (I suspect so.  It&rsquo;s not like <code>snes_spc</code> is a fast-moving target,
as its first public release is also, AFAICT, its only one.)  I don&rsquo;t want to
commit one way or the other without an answer&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where the Samples Are]]></title>
    <link href="http://softglow.github.io/blog/2013/11/07/where-the-samples-are/"/>
    <updated>2013-11-07T20:58:00-05:00</updated>
    <id>http://softglow.github.io/blog/2013/11/07/where-the-samples-are</id>
    <content type="html"><![CDATA[<p>Looking at the <a href="http://byuu.org/higan/">higan</a> v093 source, it&rsquo;s fairly
obvious where the DSP code lives, but how does the sound escape it?</p>

<h2>Sending samples to the Interface</h2>

<p>In the <del>performance</del> accuracy profile, the process actually begins down in
<code>echo_27()</code>, implemented in <code>sfc/dsp/echo.cpp</code>.  It is there that the elusive
<code>audio.sample()</code> method is called, which is part of Audio, over in
<code>sfc/system/audio.cpp</code>.  In the case where there&rsquo;s no coprocessor, it passes
directly into <code>interface-&gt;audioSample</code>.</p>

<p>In the complicated case where there <em>is</em> a coprocessor, then the audio heads
into a DSP buffer, to be averaged in <code>audio.flush()</code> with the coprocessor&rsquo;s
buffer; the final samples are clamped to 16 bits and finally routed through
the same <code>interface-&gt;audioSample</code> there.</p>

<h2>Interface</h2>

<p>Sharp eyes will have noticed the arrow operator, indicating that <code>interface</code>
is a pointer.  It&rsquo;s a pointer to the binding of the current interface, which
is to say: it&rsquo;s a <code>Interface *interface</code> where the type is <code>struct Interface :
Emulator::Interface::Bind</code>.</p>

<p>Looking from the outside in, <code>target-ethos/bootstrap.cpp</code> creates the
binding-interface, creates and appends all the known <em>system</em> emulators to a
vector, then iterates over the vector to set each system emulator&rsquo;s binding to
the single global binding that it <em>just</em> created.</p>

<p>Although <code>audioSample</code> is virtual, I haven&rsquo;t found anything yet which actually
overrides it.</p>

<h2>audioSample &amp; dspaudio</h2>

<p>The implementation of audioSample in <code>target-ethos/interface/interface.cpp</code>
does very little:</p>

<pre><code>signed samples[] = {lsample, rsample};
dspaudio.sample(samples);
while(dspaudio.pending()) {
    dspaudio.read(samples);
    audio.sample(samples[0], samples[1]);
}
</code></pre>

<p>The <code>dspaudio</code> is a <code>nall::DSP</code> declared in <code>target-ethos/ethos.cpp</code> and
implemented in <code>nall/dsp/core.hpp</code>.  Pretty much, samples go into the DSP,
they get resampled to the output sample rate, and then those results can be
read out again.</p>

<p>The trick here is that the <code>audio</code> visible in <em>this particular scope</em> is
actually <code>AudioInterface audio</code> from <code>ruby/ruby.cpp</code>.  Platform audio drivers
live under <code>ruby/audio/*.cpp</code> and <code>AudioInterface::sample</code> passes the samples
along to whatever driver happens to be connected.</p>

<h2>tl;dr</h2>

<p>If the madness hasn&rsquo;t taken me:</p>

<ol>
<li>The system DSP produces samples and passes it into the system audio chain.</li>
<li>The system audio chain passes samples into the platform interface.</li>
<li>The platform interface pumps samples through the resampler.</li>
<li>The resulting samples are pushed into the platform audio driver.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solved: The Mystery of Privilege]]></title>
    <link href="http://softglow.github.io/blog/2013/11/04/solved-the-mystery-of-privilege/"/>
    <updated>2013-11-04T19:30:00-05:00</updated>
    <id>http://softglow.github.io/blog/2013/11/04/solved-the-mystery-of-privilege</id>
    <content type="html"><![CDATA[<p>Just a quick note, since <code>privileged</code> is actually not a C++ keyword, the
thought <em>finally</em> crossed my mind: what if it&rsquo;s a macro?</p>

<p>One quick grep later, there was one match in <code>emulator/emulator.hpp</code> line
72-76 (for anyone who hasn&rsquo;t read the whole blog, this is the higan v093
source I&rsquo;m talking about):</p>

<pre><code>#if defined(DEBUGGER)
    #define privileged public
#else
    #define privileged private
#endif
</code></pre>

<p>Yeah, it looks like a good day to be humble about my programming ability.  It
only took me, like, a <em>month</em> to come up with that idea.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Brief Intermission]]></title>
    <link href="http://softglow.github.io/blog/2013/10/31/brief-intermission/"/>
    <updated>2013-10-31T21:04:00-04:00</updated>
    <id>http://softglow.github.io/blog/2013/10/31/brief-intermission</id>
    <content type="html"><![CDATA[<p>No progress is expected this week.  I&rsquo;m taking the week off to catch up on
some other things around the house.</p>

<p>I&rsquo;m disappointed, actually.  I wanted to have <em>higan</em> connected by the end of
October, not just a stupid triangle wave generator.  (Which is the easiest
waveform to program&mdash;since <code>dy/dt = C</code>, all you need is a fixed delta that
gets its sign inverted when reaching the edge of the range.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SndThread Lives]]></title>
    <link href="http://softglow.github.io/blog/2013/10/26/sndthread-lives/"/>
    <updated>2013-10-26T14:09:00-04:00</updated>
    <id>http://softglow.github.io/blog/2013/10/26/sndthread-lives</id>
    <content type="html"><![CDATA[<p>I wrote that I had the thread buffer handoff working correctly.  Hah.  Ha!
HAHAHAHAHAHA!</p>

<p><em>Now</em> it works.  I got a thin, quiet, reedy whine out of it last night, and
today, I tracked down the final bug: the IO formatter was shifting the wrong
direction, thus turning a sample like <code>3560</code> into output bytes <code>6000</code> because
<code>350000 &amp; ff</code> (the implicit conversion to byte) is zero.</p>

<p><img src="http://softglow.github.io/blog/images/2013/10/audacity-triangle-zero.png" width="780" height="620"></p>

<p>This started life as a triangle wave, but when I got a bad sound, I tried
&lsquo;flattening&rsquo; the peaks out to put in more sound power and make it fuller.
This did not work.  Not only was sound power unimportant, but instead of
holding the <em>peak</em> value, it holds <em>zero.</em></p>

<p>But, since this bug is clearly in the generator and the rest of the output
chain <strong>is</strong> working to suit me, I&rsquo;m going to leave it unfixed and move on to
connecting higan&rsquo;s SMP/DSP subsystem in place of my broken generator.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Progress Report]]></title>
    <link href="http://softglow.github.io/blog/2013/10/24/progress-report/"/>
    <updated>2013-10-24T20:02:00-04:00</updated>
    <id>http://softglow.github.io/blog/2013/10/24/progress-report</id>
    <content type="html"><![CDATA[<p>I got the buffer passing code that I described
<a href="http://softglow.github.io/blog/2013/10/20/on-qaudiooutput/">on Sunday</a> running.  It&rsquo;s important that
if you&rsquo;re not going to call <code>QIODevice::write</code> to put data into the
device&mdash;because, say, you&rsquo;re swapping in a buffer and converting it at
<code>read()</code> time because, um, you&rsquo;re just doing it wrong&mdash;that you emit the
<code>readyRead</code> and <code>bytesWritten</code> signals yourself when you swap in a buffer.
Otherwise, the audio output never gets signaled that the device is readable,
and stops output after a timeout.</p>

<p>&lt;/tangent&gt;</p>

<p>It turns out that the tone generator I&rsquo;ve connected for testing (to stand in
for higan) is broken.  Completely busted.  It produces an infinite number of
repetitions of:</p>

<pre><code>0
0
-32768
-32768
</code></pre>

<p>That is <strong>not</strong> a triangle wave.  And since no horrible howl comes out of my
speakers, either the audio output stage is also broken, or something is
detecting &ldquo;this makes no sense&rdquo; and sparing my ears.</p>

<p>Oh well.  If I&rsquo;m lucky enough, <em>maybe</em> I&rsquo;ll get to the point where I have an
SPC player using higan&rsquo;s audio engine by 2014.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On QAudioOutput]]></title>
    <link href="http://softglow.github.io/blog/2013/10/20/on-qaudiooutput/"/>
    <updated>2013-10-20T16:31:00-04:00</updated>
    <id>http://softglow.github.io/blog/2013/10/20/on-qaudiooutput</id>
    <content type="html"><![CDATA[<p>Most QT resources seem to want me to use higher-level classes like
<code>QMediaPlayer</code> or <code>QSoundEffect</code> to point at extant files or <em>possibly</em> a
<code>QAudioBuffer</code> which is already filled with a <code>QByteArray</code>.  There&rsquo;s really
not much out there about <strong>synthesizing</strong> audio and playing it straight
through a <code>QAudioDevice</code>.</p>

<p>The latter may operate in push mode or pull mode; the docs will tell you as
much.  What may be less obvious is <em>how</em> those modes are chosen.  It&rsquo;s simply
your choice of overloaded method:</p>

<pre><code>QIODevice QAudioOutput::start(); // push mode
void QAudioOutput::start(QIODevice *dev); // pull mode
</code></pre>

<p>Looking over the pre-made classes that implement <code>QIODevice</code>, I notice that
they love to tell you when <em>data is ready to read</em> but not <em>when data may be
written.</em>  And in <strong>both</strong> modes, the <code>QAudioOutput</code> is doing the reading and
your application needs to know, somehow, when it should write.</p>

<p>So, I implemented my own <code>QIODevice</code> around my own buffer type.  The class
actually holds a pair of buffers, one being written by the generator thread
and one being read by <code>QAudioOutput</code>.  When both are finished, the buffers are
swapped.  Whenever the output calls <code>read</code>, I format data from the app&rsquo;s
buffer into bytes, directly into the output pointer.</p>

<p>And actually, nobody calls write.  The generator can be connected directly to
the app buffer, which provides its own <code>append()</code> method that consumes
(potentially many) <code>snd_sample_t</code>s.</p>

<p>There are a pair of signals and slots each side uses to communicate about
their shared buffer:</p>

<pre><code>class SndIO : public QIODevice {
    Q_OBJECT
    ...
public slots:
    void writeComplete(SndBuf *buf);
signals:
    void readyWrite(SndBuf *buf);
    void emptied();
    void underrun();
}
class SndThread : public QObject {
    Q_OBJECT
    ...
public slots:
    void start(SndBuf *buf);
signals:
    void finished(SndBuf *buf);
}
</code></pre>

<p>The main thread then connects <code>SndIO::readyWrite</code> to <code>SndThread::start</code> and
<code>SndThread::finished</code> to <code>SndIO::writeComplete</code>.  The SndBuf itself isn&rsquo;t used
simultaneously by multiple threads, because the IO won&rsquo;t touch it until the
Thread has signaled that it has finished filling it.</p>

<p>This produces a four-state system, starting with &ldquo;both buffers empty&rdquo; before
the main thread wires the objects and initiates the first fill on the IO.</p>

<pre><code>1. empty/empty
2. empty/filling
3. draining/filling
4. draining/full
</code></pre>

<p>When state 2 emits finished, state 3 begins; emission of emptied there returns
to state 2, otherwise, finished happens again and state 4 is entered.  From
state 4, the only move is back to state 3 when the emptied signal is emitted.
After the initialization period (once state 3 has been entered for the first
time), it&rsquo;s possible for state 2 to receive another read from the
<code>QAudioOutput</code>; this emits the underrun signal without changing state.</p>

<p>This is a actually a slightly-more-complex version of the producer/consumer
problem.  Instead of a single producer and consumer directly communicating,
there is an intervening <code>QIODevice</code> carrying data across threads, consuming
the generator&rsquo;s output, then marshaling and producing it for the output to
consume.</p>

<p>I&rsquo;m also effectively using signals and slots to let the event loop block the
producer when it gets too far ahead.  When that happens, no &ldquo;produce more&rdquo;
signal is delivered until the output can catch up.</p>

<p>All that said, this setup <em>compiles</em> but I haven&rsquo;t finished wrapping a test
program around it yet.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[libco: Sync via Calling without Endless Recursion]]></title>
    <link href="http://softglow.github.io/blog/2013/10/13/libco-sync-via-calling/"/>
    <updated>2013-10-13T18:37:00-04:00</updated>
    <id>http://softglow.github.io/blog/2013/10/13/libco-sync-via-calling</id>
    <content type="html"><![CDATA[<p>Continuing our discussion of <a href="http://byuu.org/higan/">higan</a> from
<a href="http://softglow.github.io/blog/2013/10/12/synchronization-in-higan-smp-slash-dsp/">yesterday</a>, let’s
take a moment to look deeper into the abyss.</p>

<h2>SMP::Threaded</h2>

<p>This isn’t pthreads or anything; it’s <code>libco</code> threads.  libco appears to be
named for <em>coroutines</em> and focuses on user-space or “green” threading rather
than OS threads.  In fact, libco ships with generic implementations for
Windows using fibers, and for GCC using a GNU Pth inspired setjmp/longjmp
technique.  Including a little trickery with <code>sigaltstack</code> to actually set up
the alternate allocation as a stack.</p>

<p>AFAICT, everything is set up with <code>Threaded=true</code> and the emulator would
actually break if it were otherwise.  Everything would keep calling
<code>thing.enter()</code> until eventually the emulator ran out of stack space and
crashed.</p>

<h2>Tracing cpu.add_clocks</h2>

<p>From our previous post, we know that <code>add_clocks</code> is called when the CPU needs
to perform bus operations, so our adventure today really begins with the
implementation of this function, inside <code>sfc/cpu/timing/timing.cpp</code>.</p>

<p>The function starts off by unlocking IRQs, then computing a number of ticks
(which is just <code>clocks/2</code>), and calling <code>tick()</code> for each tick.  tick itself
merely updates the horizontal and vertical counters, essentially keeping track
of where the raster scanout is happening in the PPU.  (It was hard to find;
it’s in <code>sfc/ppu/counter/counter-inline.hpp</code> which <code>CPU</code> inherits, and it’s
loaded in via <code>sfc/sfc.hpp</code> and <code>sfc/cpu/cpu.cpp</code>.)</p>

<p>After ticking, the step function is called, which reduces the clock cycle of
each active subsystem (SMP, PPU, and any coprocessors present) before
synchronizing the controllers.  Next, the joysticks are polled every 256
clocks, DRAM refreshed when needed (at a cost of 40 more cycles), and when
<code>DEBUGGER</code> is defined, all the other chips are fully synchronized again.</p>

<h2>clocks, then&hellip;</h2>

<p>It appears that the clocks are tracking “cycles ahead/behind in emulation:”
where “0” means perfectly synchronized with the current CPU state, negative
means the given part is behind, and of course, positive means ahead.  When the
CPU runs, it reduces the <code>smp.clock</code> and <code>dsp.clock</code> values, and when the SMP
gets a chance to run, it increases its <code>smp.clock</code> value.  When it crosses 0,
then the SMP is synchronized, and if the synchronization mode calls for it,
emulation will sync up the DSP (by the same mechanism: the SMP has been
decrementing clocks, so when sync happens, the DSP increments its clock
value), then return to the CPU.</p>

<h2>Synchronization points</h2>

<p>The SMP is actually invoked to re-sync itself with the main CPU whenever the
chips need to communicate (i.e. if the CPU wants to read or write the SMP IO
ports), on each new scanline (when tick brings the hcounter back to 0), and on
a debug build, every time the CPU is updated.  I presume that last one keeps
everything in perfect sync from the debugger’s point of view.</p>

<p>The CPU calls to <code>synchronize_smp()</code> in <code>sfc/cpu/mmio/mmio.cpp</code> (for port
access) and <code>sfc/cpu/timing/timing.cpp</code> for the other cases.</p>

<p>Meanwhile, the SMP subsystem returns the favor by calling <code>synchronize_cpu</code>
from <code>SMP::add_clocks</code> in <code>sfc/smp/timing.cpp</code>—either every sample produced in
debug builds, or after 24 samples—and from <code>sfc/smp/memory.cpp</code> on port IO.</p>

<h2>Wait, your post’s title was libco, wtf dude?</h2>

<p>The synchronization points listed above bring us back to libco.  If the CPU
calls <em>into</em> the SMP to synchronize it, and it calls <em>into</em> the CPU afterward,
then the call stack is going to get infinitely deep.  In this contrived case
where I ignore a whole lot of stuff (both the other hardware, and the rest of
the call stack involved in actually emulating the SMP/CPU between calls to
enter), the stack would get filled up looking like:</p>

<pre><code>inside smp.enter
inside cpu.enter
inside smp.enter
inside cpu.enter
inside smp.enter
inside cpu.enter
…
</code></pre>

<p>When it eventually exhausted the stack limit, the process would receive a
segfault and most likely, unceremoniously die after emulating a finite number
of cycles.  That’s where <code>libco</code> comes in.</p>

<p>Because <code>Threaded</code> is always true, direct calls aren’t made; they’re sent
through <code>co_switch</code> instead, which does some trickery to swap out the “thread”
that is running the active processor, and replace it with the thread of the
target.</p>

<p>Instead of pushing a frame onto the <em>current</em> stack and starting the target
function anew, <code>libco</code> allows for suspending the current thread+stack
entirely, (re)loading the other one, and resuming it—exactly as if <em>it</em> had
called and returned, except this appears in C++ as a call and call.</p>

<p>This gives byuu a lot of freedom to just call <code>synchronize_thing</code> all over the
place without worrying, because it’s not consuming a limited amount of call
depth to do so.  But it makes it a little harder to just yank out the code and
call it from a QT program.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Synchronization in higan (SMP/DSP)]]></title>
    <link href="http://softglow.github.io/blog/2013/10/12/synchronization-in-higan-smp-slash-dsp/"/>
    <updated>2013-10-12T16:04:00-04:00</updated>
    <id>http://softglow.github.io/blog/2013/10/12/synchronization-in-higan-smp-slash-dsp</id>
    <content type="html"><![CDATA[<h2>Background Matter</h2>

<p>In case you missed my master plan (e.g.: you&rsquo;re reading my Github and were
never on metconst), I&rsquo;m trying to build a music editor for, at the very least,
SPC files from Super Metroid.</p>

<p>It seems like the fastest way for such an editor to be able to <em>play</em> the
music being edited, is to have a real SPC player built-in.  But not one that
dumps straight to the platform audio output device&mdash;one that buffers into RAM
to make play/pause and seeking possible.</p>

<p>To that end, I&rsquo;m presently trying to rip the SMP/DSP subsystem out of
<a href="http://byuu.org/higan/">higan</a> (née bsnes) and pull it into a library that
runs in a subthread and lets the main thread request blocks of audio from it.</p>

<p>Oh yes, I&rsquo;m going to be building in QT.  I may, or may not, eventually borrow
some source code from <code>qtractor</code> (certainly not the audio output chain,
though; AFAIK, if you don&rsquo;t really want audio to play, then JACK is an
excellent backend for you.)  QT appears to be the one cross-platform library
that&rsquo;s monolithic enough to be an easy (if fat) install on Windows, that also
provides everything I absolutely need: threads, audio, GUI widgets, and a C++
FFI for binding to higan&rsquo;s SMP core.</p>

<h2>Higan v093</h2>

<p>So with that out of the way, let&rsquo;s take a closer look at higan.  I&rsquo;m
specifically using v093 here, which is, as of late September 2013, the latest
version published.  I hear it even builds on OS X now.</p>

<p>Why higan?  Accuracy.  There doesn&rsquo;t seem to be any point in going for
anything less, when I won&rsquo;t have competition for host time from the CPU and
PPU.  I&rsquo;ve also heard (but been unable to find any trace of the actual <em>code</em>
involved) that the SMW community came up with some music editor(s) once upon a
time, but they were based on the ZSNES emulation, and failed hard on the real
hardware because ZSNES cheated on echo buffering.  Echo didn&rsquo;t get written
back to PSRAM and destroy the code/instruments under emulation.</p>

<p>That&rsquo;s a mistake that&rsquo;s worth not repeating.</p>

<h3>Synchronization</h3>

<p>higan does threading (when threaded) by using its built-in public domain
<code>libco</code> (coroutine, I presume) library.  But if you just look at
<code>sfc/smp/smp.cpp</code> you will find some synchronization functions, but no
callers&hellip; right there.  And the obvious ones like <code>step()</code> don&rsquo;t seem to do
much.</p>

<p>There are some more functions in <code>sfc/smp/timing/timing.cpp</code>, but that turns
out to be a <em>very small</em> file, because it&rsquo;s just providing some more sync
functions to be called.  The real synchronization is driven from the
<strong>memory</strong> files, <code>sfc/smp/memory/memory.cpp</code>.  Quoting my notes:</p>

<pre><code>step(clocks): adjusts clocks only, no sync
cycle_edge: tick SMP timers
add_clocks: step(); sync DSP; may sync CPU
...
add_clocks called in op_{io|read|write}
</code></pre>

<p>It appears that higan&rsquo;s philosophy is to synchronize <em>the bus state</em> of the
system.  That is, when a CPU (the S-SMP in the case of audio) makes some sort
of move like loading the next instruction from memory, it triggers a
synchronization with the rest of the system.  However many cycles the DSP
<em>should have taken</em> while that one SMP instruction was running becomes
accounted for, and the DSP state is &ldquo;caught up&rdquo; so that its <em>current</em> bus
state will be visible to the emulation of the SMP when the thread returns.</p>

<p>That&rsquo;s pretty much how the main CPU and the SMP are sync&#8217;ed, as well: whenever
either side writes to an IO port connecting them, at end-of-scanline, and any
time they get more than 24 samples of audio output out-of-sync.  Or, at every
bus transaction, if the debugger is built (or maybe enabled?  I forget which.)</p>

<p>I didn&rsquo;t look, but I&rsquo;d guess that&rsquo;s how the CPU and PPU are wired, as well.</p>

<h3>struct Everything</h3>

<p>Apparently in the C++ world, a <code>struct</code> is a <code>class</code> with default-public
instead of default-private.  byuu uses a lot of <code>struct</code> and very few, if
<em>any</em>, <code>class</code> keywords.</p>

<p>Why you would use C-compatible keywords to build non-C features?  I&rsquo;m baffled.
It&rsquo;s just how Bjarne rolls, I guess.</p>

<h3>The Mystery of Privilege</h3>

<p>There&rsquo;s apparently a &ldquo;privileged&rdquo; access modifier.  How this may differ from
<code>private</code> or <code>protected</code>, I&rsquo;m not really certain, because so far I&rsquo;ve been
unable to find a description of it online.  It&rsquo;s just too generic a word:
every discussion of <code>public</code> vs. <code>private</code> tends to use it to talk about
access control.</p>

<p>Unfortunately, there&rsquo;s a good chance I may <em>need</em> to understand it before
finishing this library.</p>
]]></content>
  </entry>
  
</feed>
